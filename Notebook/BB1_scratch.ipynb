{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8bb3f07-8b0a-4ddc-b5b7-04b4d823f759",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Legacy Way of creating the Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6610f2-a6aa-49ed-b902-de3dc3442be0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2728672862096326>:4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"  Here we are importing only selected functions / class from library  \"\"\"\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n",
       "\u001B[0;32m----> 4\u001B[0m sc\u001B[38;5;241m=\u001B[39mSparkContext()\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SQLContext\n",
       "\u001B[1;32m      6\u001B[0m sqlc\u001B[38;5;241m=\u001B[39mSQLContext(sc)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    200\u001B[0m     )\n",
       "\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n",
       "\u001B[1;32m    205\u001B[0m         master,\n",
       "\u001B[1;32m    206\u001B[0m         appName,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n",
       "\u001B[1;32m    217\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n",
       "\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n",
       "\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    493\u001B[0m             currentAppName,\n",
       "\u001B[1;32m    494\u001B[0m             currentMaster,\n",
       "\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n",
       "\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n",
       "\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n",
       "\u001B[1;32m    498\u001B[0m         )\n",
       "\u001B[1;32m    499\u001B[0m     )\n",
       "\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-2728672862096326>:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"  Here we are importing only selected functions / class from library  \"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n\u001B[0;32m----> 4\u001B[0m sc\u001B[38;5;241m=\u001B[39mSparkContext()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SQLContext\n\u001B[1;32m      6\u001B[0m sqlc\u001B[38;5;241m=\u001B[39mSQLContext(sc)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Legacy way (Old way of creating the SPark and SQL Context ) :         deprecated \n",
    "\"\"\"  Here we are importing only selected functions / class from library  \"\"\"\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc=SQLContext(sc)\n",
    "from pyspark.sql import HiveContext\n",
    "hc=SparkContext()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12460944-8efb-4117-951b-be55b1a2fd79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Modern Way of creating the Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2122be3-97ba-4ddf-b7dc-a22a4770910e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/context.py:734: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Modern way used in recent days \n",
    "#from pyspark.sql.session import *\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"BB1 CLuster \").enableHiveSupport().getOrCreate()\n",
    "\n",
    "''' from SparkSession we are accessing the builder function which need the master and parameter passed as local[2] (That is running in local mode and with 2 core cluster ) ''' \n",
    "#and we are setting the appName of the application  \n",
    "# at last enabling Hive Support and we are creating  \n",
    "\n",
    "sc = spark.sparkContext  #Spark COntext created \n",
    "sql_context=SQLContext(sc)\n",
    "hive_context=HiveContext(sc)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0077ec-96a6-473b-a840-9a1e30779808",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create a folder for holding DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e64e83-1079-41a9-93c5-2b5d083f6810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB1/Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69272bd7-b8bd-4780-83ed-502ad1914fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "mkdirs /dbfs/FileStore/BB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf06523-6ce1-48da-88f0-a983f3a75445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Delete a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5305d7be-6314-47c5-845e-49f7941912ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "# Delete a folder in DBFS\n",
    "dbutils.fs.rm(\"/dbfs/FileStore/BB1\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51295011-4150-46b1-b1b7-be5b9891d3c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Listing the  files below is the wrong way of doing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4aa8fa-7940-4aaf-a83d-d89acdb46f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:1)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:43)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:45)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:47)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw.&lt;init&gt;(command-2728672862096339:49)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw.&lt;init&gt;(command-2728672862096339:51)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read.&lt;init&gt;(command-2728672862096339:53)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$.&lt;init&gt;(command-2728672862096339:57)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$.&lt;clinit&gt;(command-2728672862096339)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1298)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1251)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$25(DriverLocal.scala:904)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:895)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:872)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:1)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:43)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:45)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw$$iw.&lt;init&gt;(command-2728672862096339:47)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw$$iw.&lt;init&gt;(command-2728672862096339:49)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$$iw.&lt;init&gt;(command-2728672862096339:51)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read.&lt;init&gt;(command-2728672862096339:53)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$.&lt;init&gt;(command-2728672862096339:57)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$read$.&lt;clinit&gt;(command-2728672862096339)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linedff9e66931a84aa0b90da5508f6374f139.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1298)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1251)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$25(DriverLocal.scala:904)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:895)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:872)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /dbfs/FileStore/data",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /dbfs/FileStore/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1718ee38-c316-483a-9be3-7ef7a329b0c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Listing the  files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24aedf0f-32e7-4981-83f4-8ad2037e5f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/data/custsmodified</td><td>custsmodified</td><td>381413</td><td>1719646728000</td></tr><tr><td>dbfs:/FileStore/data/txns</td><td>txns</td><td>8472053</td><td>1719726596000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/data/custsmodified",
         "custsmodified",
         381413,
         1719646728000
        ],
        [
         "dbfs:/FileStore/data/txns",
         "txns",
         8472053,
         1719726596000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8138dd18-7921-456c-9c21-eeda289cb747",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Delete a File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7b33ab-b973-43d5-96ce-8fbf1e6090c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/data/txns-2\")\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/data/txns-1\")\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/nyse.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e392396-02bc-4377-a4d8-51193055cc68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create necessary RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a4bd54-42c6-4290-880d-a02a1a451fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYSE~CLI~35.3', 'NYSE~ABC~24.62', 'NYSE~CAB~33.2']\n[['NYSE', 'CLI', '35.3'], ['NYSE', 'ABC', '24.62'], ['NYSE', 'CAB', '33.2']]\n[('NYSE', 'CLI', 35.3), ('NYSE', 'ABC', 24.62), ('NYSE', 'CAB', 33.2)]\n"
     ]
    }
   ],
   "source": [
    "#rdd_1 = sc.textFile(\"/dbfs/FileStore/BB1/Data/nyse.csv\")  as the RDD is lazy execution the file not found error while take on RDD  \n",
    "                        #or\n",
    "rdd_1 = sc.textFile(\"dbfs:/FileStore/BB1/Data/nyse.csv\")\n",
    "storage = rdd_1.take(10);\n",
    "print(storage)\n",
    "#because of partition the number of jobs may vary \n",
    "\n",
    "#Below we are going to split the ROW by ~\n",
    "#MAP is like for while loop \n",
    "splited_rdd=rdd_1.map(lambda split_var : split_var.split(\"~\"))\n",
    "print(splited_rdd.take(3))\n",
    "\n",
    "#After spliting typecasting is done list to tuple and last col is float now  \n",
    "final_splited_rdd_1 =splited_rdd.map(lambda lsplited_rdd : (lsplited_rdd[0] , lsplited_rdd[1] , float(lsplited_rdd[2])) )\n",
    "print(final_splited_rdd_1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b681e1d7-5c57-4e1f-afd2-ae2d3ea12517",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The above task done using a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775aac33-8762-4a49-b074-c6834c7ec3c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|EXCHANGE|STOCK|PRICE|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n+--------+-----+-----+\n|EXCHANGE|STOCK|PRICE|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\nOut[9]: <bound method DataFrame.describe of DataFrame[EXCHANGE: string, STOCK: string, PRICE: float]>"
     ]
    }
   ],
   "source": [
    "data_frame = spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , inferSchema=True).toDF(\"EXCHANGE\" , \"STOCK\" , \"PRICE\")\n",
    "data_frame.show(3)\n",
    "data_frame.describe\n",
    "#IF we are not using inferSchema \\\n",
    "from pyspark.sql.types import *\n",
    "custom_schema = StructType([StructField(\"EXCHANGE\",StringType(),True),StructField(\"STOCK\",StringType(),True),StructField(\"PRICE\",FloatType(),True) ])\n",
    "data_frame_1 = spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" ,schema=custom_schema)\n",
    "data_frame_1.show(3)\n",
    "data_frame_1.describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f3da55-dc54-447d-af63-78f027ebc46d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Convert the RDD to DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aefc487-40a7-4976-9e84-f57fd2d780ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Way 1 \n+----+---+-----+\n|  _1| _2|   _3|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\nWay 2 \n+--------+-----+-----+\n|EXCHANGE|STOCK|PRICE|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\nWay 3 \n+--------+-----+-----+\n|EXCHANGE|STOCK|PRICE|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#The below is not working \n",
    "#RDD_to_DF = final_splited_rdd_1.toDF(\"EXCHANGE\",\"STOCK\",\"PRICE\") \n",
    "#RDD_to_DF.show(2)\n",
    "              #or \n",
    "RDD_to_DF_1 = final_splited_rdd_1.toDF() \n",
    "print(\"Way 1 \")\n",
    "RDD_to_DF_1.show(3)\n",
    "              #or\n",
    "column_header = (\"EXCHANGE\",\"STOCK\",\"PRICE\")\n",
    "RDD_to_DF_2 = final_splited_rdd_1.toDF(column_header)\n",
    "print(\"Way 2 \")\n",
    "RDD_to_DF_2.show(3)\n",
    "              #or\n",
    "column_header_1 = (\"EXCHANGE\",\"STOCK\",\"PRICE\")\n",
    "RDD_to_DF_3 = spark.createDataFrame(final_splited_rdd_1 , column_header)\n",
    "print(\"Way 3 \")\n",
    "RDD_to_DF_3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9313f3-454f-4ed6-a08d-f17b89785255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###write SQL or DSL queries lavishly , rather than writing RDD transformations/actions on the schema_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c68f553-5ba8-4354-b264-7950d317d54f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter only the NYSE as EXCHANGE VALUE \n",
    "RDD_to_DF_2.createOrReplaceTempView(\"table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31a54cf-1566-4e06-8cb2-84eb5f521138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EXCHANGE</th><th>STOCK</th><th>PRICE</th></tr></thead><tbody><tr><td>NYSE</td><td>CLI</td><td>35.3</td></tr><tr><td>NYSE</td><td>ABC</td><td>24.62</td></tr><tr><td>NYSE</td><td>CAB</td><td>33.2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "NYSE",
         "CLI",
         35.3
        ],
        [
         "NYSE",
         "ABC",
         24.62
        ],
        [
         "NYSE",
         "CAB",
         33.2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EXCHANGE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "STOCK",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PRICE",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from table where EXCHANGE == \"NYSE\" limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28edaef0-2101-40d0-a381-8083c0114be0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2728672862096353>:8\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m string_passed \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124m select * from table where EXCHANGE == \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNYSE\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m df_2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(string_passed);\n",
       "\u001B[0;32m----> 8\u001B[0m rdd_2\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m3\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'rdd_2' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2728672862096353>:8\u001B[0m\n\u001B[1;32m      6\u001B[0m string_passed \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124m select * from table where EXCHANGE == \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNYSE\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      7\u001B[0m df_2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(string_passed);\n\u001B[0;32m----> 8\u001B[0m rdd_2\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m3\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'rdd_2' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'rdd_2' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_2 = select * from table where EXCHANGE == \"NYSE\";\n",
    "#the above cant be done so do the below \n",
    "                            #so\n",
    "df_2 = spark.sql(\"select * from table where EXCHANGE == 'NYSE' \");\n",
    "                            #so\n",
    "string_passed = \"\"\" select * from table where EXCHANGE == \"NYSE\" \"\"\"\n",
    "df_2 = spark.sql(string_passed);\n",
    "rdd_2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d187c09-50a0-4df7-9834-9bf2dee0e3cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Acheveing the above by DSL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d88a1a-75c5-45ff-bc6a-cade737a700b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|EXCHANGE|STOCK|PRICE|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#df_3 = RDD_to_DF_2.where(\"EXCHANGE in 'NYSE'\").select(EXCHANGE,STOCK,PRICE)\n",
    "#above we have not given the col name in quotes \n",
    "\n",
    "df_3 = RDD_to_DF_2.where(\"EXCHANGE == 'NYSE'\").select(\"EXCHANGE\", \"STOCK\", \"PRICE\")\n",
    "df_3.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004c0505-a774-4377-b070-a9e1067da522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create another DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44395070-6aa8-47aa-8c83-c612c418f398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+-----------+-----+-----------+\n|         c1|    c2|    c3|         c4|   c5|         c6|\n+-----------+------+------+-----------+-----+-----------+\n|     hadoop| spark|hadoop|      spark|kafka|datascience|\n|      spark|hadoop| spark|datascience| null|       null|\n|informatica|  java|   aws|        gcp| null|       null|\n+-----------+------+------+-----------+-----+-----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_4 = spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/courses.log\" , sep=\" \" ,inferSchema=True).toDF(\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\")\n",
    "df_4.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06b6567-7a6a-450e-86fc-97a810457d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Do the above by the RDD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4204efdd-8585-4c05-830d-abd374358679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hadoop spark hadoop spark kafka datascience', 'spark hadoop spark datascience', 'informatica java aws gcp']\n"
     ]
    }
   ],
   "source": [
    "rdd_2=sc.textFile(\"dbfs:/FileStore/BB1/Data/courses.log\") \n",
    "print(rdd_2.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900e9362-531e-47bc-a599-7807e17be834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hadoop', 'spark', 'hadoop', 'spark', 'kafka', 'datascience'], ['spark', 'hadoop', 'spark', 'datascience']]\n[('hadoop', 1), ('spark', 1), ('hadoop', 1), ('spark', 1), ('kafka', 1), ('datascience', 1), ('spark', 1), ('hadoop', 1), ('spark', 1), ('datascience', 1), ('informatica', 1), ('java', 1), ('aws', 1), ('gcp', 1), ('gcp', 1), ('aws', 1), ('azure', 1), ('spark', 1), ('gcp', 1), ('pyspark', 1), ('hadoop', 1), ('hadoop', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Checking for the difference between the map and flatmap \n",
    "mapped_rdd_2=rdd_2.map(lambda x:x.split(\" \"))\n",
    "print(mapped_rdd_2.take(2))\n",
    "\n",
    "#The below flatMap is for doing 2 level of iterating \n",
    "#Word count program \n",
    "mapped_rdd_2=rdd_2.flatMap(lambda x:x.split(\" \")).map(lambda x: (x,1))\n",
    "print(mapped_rdd_2.take(234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047fe7ba-27f6-46ff-aa58-50f4b0aa15db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', 5), ('java', 1), ('aws', 2), ('azure', 1), ('spark', 5), ('kafka', 1), ('datascience', 2), ('informatica', 1), ('gcp', 3), ('pyspark', 1)]\n"
     ]
    }
   ],
   "source": [
    "#understand the reducebykey for summing it up (count the occurence of the word )\n",
    "#reduceByKey operation sums up the values for each key\n",
    "count_rdd=mapped_rdd_2.reduceByKey(lambda x,y:x+y)\n",
    "print(count_rdd.take(23454))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63686ccc-f265-4e4c-9d87-367f3270a4aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hadoop spark hadoop spark kafka datascience', 'spark hadoop spark datascience', 'informatica java aws gcp']\n[('hadoop',), ('spark',), ('hadoop',), ('spark',), ('kafka',), ('datascience',), ('spark',), ('hadoop',), ('spark',), ('datascience',), ('informatica',), ('java',), ('aws',), ('gcp',), ('gcp',), ('aws',), ('azure',), ('spark',), ('gcp',), ('pyspark',), ('hadoop',), ('hadoop',)]\n"
     ]
    }
   ],
   "source": [
    "#flatMap understanding \n",
    "rdd_2=sc.textFile(\"dbfs:/FileStore/BB1/Data/courses.log\") \n",
    "print(rdd_2.take(3))\n",
    "rdd_3=rdd_2.flatMap(lambda x:x.split(\" \")).map(lambda x:(x,))\n",
    "print(rdd_3.take(12456))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c37fe47-7b95-41bb-8330-2bfca8dbd0e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create a RDD for the above \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941e6dc0-4b25-42b6-b911-871a6f9afd0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|Course_Name|\n+-----------+\n|     hadoop|\n|      spark|\n|     hadoop|\n|      spark|\n|      kafka|\n|datascience|\n|      spark|\n|     hadoop|\n|      spark|\n|datascience|\n|informatica|\n|       java|\n|        aws|\n|        gcp|\n|        gcp|\n|        aws|\n|      azure|\n|      spark|\n|        gcp|\n|    pyspark|\n|     hadoop|\n|     hadoop|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#THe below is error \n",
    "#df_3=rdd_3.toDF(\"Course_Name\") \n",
    "#df_3.show(245)\n",
    "\n",
    "column = [\"Course_Name\"]\n",
    "df_3=rdd_3.toDF(column) \n",
    "df_3.show(245)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5bd6ad-53f8-46cc-8847-0fe9c0ab32eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The Below is the SQL way of writting word count after spliting is done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fcec85b-b7d4-4bb2-af98-1febcc70da17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|Course_Name|count|\n+-----------+-----+\n|      kafka|    1|\n|      spark|    5|\n|     hadoop|    5|\n|datascience|    2|\n|      azure|    1|\n|informatica|    1|\n|        gcp|    3|\n|    pyspark|    1|\n|       java|    1|\n|        aws|    2|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_3.createOrReplaceTempView(\"table\")\n",
    "temp_df=spark.sql(\"select Course_Name,count(*) as count from table group by Course_Name \")\n",
    "temp_df.show(234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b283bb5-1661-4dc6-b1b0-2470696198a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## -------creating DF from rdd - ends here-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce415ed-2a6d-42fb-abf5-0e946b23a5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Below agenda \n",
    "###creating DF directly from different sources using different builtin functions using different (important) options using different methodologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c482ef47-b72a-4d66-bf7c-7fc5aa519c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+-----------+-----+-----------+\n|        _c0|    _c1|   _c2|        _c3|  _c4|        _c5|\n+-----------+-------+------+-----------+-----+-----------+\n|     hadoop|  spark|hadoop|      spark|kafka|datascience|\n|      spark| hadoop| spark|datascience| null|       null|\n|informatica|   java|   aws|        gcp| null|       null|\n|        gcp|    aws| azure|      spark| null|       null|\n|        gcp|pyspark|hadoop|     hadoop| null|       null|\n+-----------+-------+------+-----------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_2=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/courses.log\" , sep=\" \" , inferSchema=True)\n",
    "data_frame_2.show(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e877da9-1f35-45ee-8f24-6b9aa08da766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_3=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , inferSchema=True)\n",
    "data_frame_3.show(234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722abd95-1bd8-4827-ac7d-52d3a29a9c56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Below is the diff between header=True and header=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9b6acb-52ba-40cb-b44e-eff273035f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n|NYSE|CLI| 35.3|\n+----+---+-----+\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_4=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , inferSchema=True , header=True)\n",
    "data_frame_4.show(234)\n",
    "                                #or\n",
    "data_frame_5=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , inferSchema=True , header=False)\n",
    "data_frame_5.show(234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f16899f-46c2-4306-bfcf-d7baa2df4ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###In RDD creation Important options are - delimiter, inferschema, header and  Important options are - schema, mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a3bf68-65b0-44ea-9a2c-7c403cb14a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Option 4 : schema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60ef061-8050-411f-8ec4-e3bfffab851b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "custom_schema=StructType([StructField(\"Exchange\",StringType(),True) , StructField(\"Stock\" ,StringType(),True) , StructField(\"Price\" , FloatType(),True )])\n",
    "#data_frame_6=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=\"custom_schema\")\n",
    "data_frame_6=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema)\n",
    "data_frame_6.show(343) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6aa2b28-d5df-4b03-bab4-bc6a202b56db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Option mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757f44e0-4e47-43d8-b1a5-7348ef53a98c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mode=\"Permissive\" \n",
    "Default mode is permissive()\n",
    "If anything is not supported put the Null and the data is permitted and Null is printed \n",
    "\"\"\"\n",
    "\n",
    "data_frame_7=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema, mode=\"permissive\")\n",
    "data_frame_7.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc081f3-0c5b-4c4c-ae95-8d75f501de52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mode=\"dropmalformated\"\n",
    "It drops the wrong / mistyped data (row wise) \n",
    "\"\"\"\n",
    "\n",
    "data_frame_8=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema, mode=\"dropmalformated\")\n",
    "data_frame_8.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0a01c6-cf7e-490c-a25c-aade214b9706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mode=\"failfast\"\n",
    "If data is not OK based on our schema the error occurs .\n",
    "\"\"\"\n",
    "data_frame_9=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema, mode=\"failfast\")\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a6360e-f539-4338-a3bc-fccc02d52707",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##InferSchema is used when the data is in small volume and  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddc825d-bafe-4774-9ad3-aaf1f7a695ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###3. Different way/methodologies to define dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab828768-b930-4159-86a5-8ed0082d9775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#inline \n",
    "data_frame_9=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema, mode=\"failfast\")\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eff3ec6-4d9b-4aba-a29c-5c1a1636dbc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#options\n",
    "data_frame_9=spark.read.options(sep=\"~\" , schema=custom_schema,mode=\"failfast\").csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\")\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccce854b-b8c2-47b8-88c2-870c329e266e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#option\n",
    "custom_schema=StructType([StructField(\"Exchange\",StringType(),True) , StructField(\"Stock\" ,StringType(),True) , StructField(\"Price\" , FloatType(),True )])\n",
    "data_frame_9=spark.read.option(\"sep\",\"~\").option(\"schema\",\"custom_schema\").csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\")\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b533fd-f291-4b84-a712-53e4d2e3f0f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n|NYSE|CLI| 35.3|\n+----+---+-----+\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#format and load \n",
    "data_frame_9 = spark.read.format(\"csv\").option(\"delimiter\", \"~\").option(\"header\", \"true\").load(\"dbfs:/FileStore/BB1/Data/nyse.csv\")\n",
    "\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0795b413-e531-4e85-a2e8-26794784f9d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#load and format inline  \n",
    "data_frame_9 = spark.read.option(\"header\",\"False\").load(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\",format=\"csv\",sep =\"~\")\n",
    "\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242e9608-a7ed-4320-b1aa-18607572b203",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Summary of above \n",
    "### Options - delimiter/sep, header, inferSchema,schema ( Custom schema), Mode ( persmissive, dropmalformed, failfast)\n",
    "### Methodologies - option(opt1,value1), options(opt1=value,opt2=value), csv(filename,opt1=value,opt2=value),\n",
    "### format(csv).option/options.load(\"file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c54e09-3df3-47f2-a73d-9f709b5f5cb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Agenda below to be done \n",
    "###Create dataframes using the modules (builtin (csv, orc, parquet, jdbc,table, json) / external later (cloud, api, nosql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1b474f-b9d2-4770-9a1b-99f1d8800068",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#CSV \n",
    "data_frame_9=spark.read.csv(path=\"dbfs:/FileStore/BB1/Data/nyse.csv\" , sep=\"~\" , schema=custom_schema, mode=\"failfast\")\n",
    "data_frame_9.show(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e9f018-5fc5-4a66-b3cb-9a040e7e2210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-418583315799616>:4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m custom_schema\u001B[38;5;241m=\u001B[39mStructType([StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExchange\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m) , StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStock\u001B[39m\u001B[38;5;124m\"\u001B[39m ,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m) , StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m , FloatType(),\u001B[38;5;28;01mTrue\u001B[39;00m )])\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#data_frame_10 = spark.read.json(\"dbfs:/FileStore/BB1/Data/nyse_json\" , schema=custom_schema )\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m data_frame_10 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/BB1/Data/nyse_json\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
       "\u001B[1;32m      5\u001B[0m data_frame_10\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m124\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m data_frame_10\u001B[38;5;241m.\u001B[39mdescribe\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:420\u001B[0m, in \u001B[0;36mDataFrameReader.json\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001B[0m\n",
       "\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\u001B[1;32m    419\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n",
       "\u001B[1;32m    423\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator: Iterable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterable:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/BB1/Data/nyse_json."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-418583315799616>:4\u001B[0m\n\u001B[1;32m      2\u001B[0m custom_schema\u001B[38;5;241m=\u001B[39mStructType([StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExchange\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m) , StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStock\u001B[39m\u001B[38;5;124m\"\u001B[39m ,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m) , StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m , FloatType(),\u001B[38;5;28;01mTrue\u001B[39;00m )])\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#data_frame_10 = spark.read.json(\"dbfs:/FileStore/BB1/Data/nyse_json\" , schema=custom_schema )\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m data_frame_10 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/BB1/Data/nyse_json\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m      5\u001B[0m data_frame_10\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m124\u001B[39m)\n\u001B[1;32m      6\u001B[0m data_frame_10\u001B[38;5;241m.\u001B[39mdescribe\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:420\u001B[0m, in \u001B[0;36mDataFrameReader.json\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001B[0m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    419\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator: Iterable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterable:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/BB1/Data/nyse_json.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/BB1/Data/nyse_json.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#JSON\n",
    "custom_schema=StructType([StructField(\"Exchange\",StringType(),True) , StructField(\"Stock\" ,StringType(),True) , StructField(\"Price\" , FloatType(),True )])\n",
    "#data_frame_10 = spark.read.json(\"dbfs:/FileStore/BB1/Data/nyse_json\" , schema=custom_schema )\n",
    "data_frame_10 = spark.read.json(\"dbfs:/FileStore/BB1/Data/nyse_json\" )\n",
    "data_frame_10.show(124)\n",
    "data_frame_10.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c084a336-09d3-4440-9765-89f95f7fae47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Serialized builtin data format (Orc/Parquet)\n",
    "###Parquet Module (Spark preferred format)- Serialized data (optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf183128-d0a1-4528-9ca0-2198392b69b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##We are storing in the Parquet or ORC because it is a best format for storing the raw / stagging layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811322b5-c389-43c2-97b9-486447edf6a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Spark Read "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85310835-fc61-42b1-8a71-39e18d6152f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491bac70-559e-4ac9-9b33-7151ace7995c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+----------+--------+----------+----+-------+-----+\n| auctionid|  bid|        bidder|bidderrate| bidtime|daystolive|item|openbid|price|\n+----------+-----+--------------+----------+--------+----------+----+-------+-----+\n|8213034705| 95.0|      jake7870|         0|2.927373|         3|xbox|   95.0|117.5|\n|8213034705|115.0| davidbresler2|         1|2.943484|         3|xbox|   95.0|117.5|\n|8213034705|100.0|gladimacowgirl|        58|2.951285|         3|xbox|   95.0|117.5|\n+----------+-----+--------------+----------+--------+----------+----+-------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_4=spark.read.json(\"dbfs:/FileStore/BB1/JSON/part1.json\") \n",
    "data_frame_4.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9bc24fa-b278-456e-bbaf-3fb29f4dc1b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1088af8-1647-4275-936c-42ff84514187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_5=spark.read.csv(\"dbfs:/FileStore/BB1/Data/nyse.csv\",sep=\"~\")\n",
    "data_frame_5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb80cb5-ec3b-4018-bc7a-f7194a9992f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res7: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res7: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "mkdirs  dbfs:/FileStore/BB1/PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c7aaf3-76f1-4cff-96b1-12fa4b01903d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b33dba-0d28-47f7-bcd0-cce573e80b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n|age|   agecat|\n+---+---------+\n|  8|childrens|\n|  9|childrens|\n| 10|childrens|\n+---+---------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_6=spark.read.parquet(\"dbfs:/FileStore/BB1/PARQUET/part1.parquet\",sep=\"~\")\n",
    "data_frame_6.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6681cd5f-5ddf-46df-9364-217925cc0259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Spark Write "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d400748-2625-4e1a-8bbe-75f6fe7a3531",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##CSV Write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8a3b37-f8b5-4677-8b1e-8d31b60ca678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_frame_6.write.csv(\"dbfs:/FileStore/BB1/Data\",header=True,sep='|',mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a5310f-4259-4b8d-8e21-95e030c64c92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Parquet Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ac6352-b286-4e94-bad9-d81b286ef436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_frame_6.write.parquet(\"dbfs:/FileStore/BB1/Data/Parquet_data\", mode=\"overwrite\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d791d94-c45a-4331-801d-c30504a18a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Interview Question - I want to write the output files into one or more, how to control the number of output files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b7812b-7db6-4693-8dee-43aebab0d0db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_frame_5.coalesce(1).write.csv(\"dbfs:/FileStore/BB1/CSV/DATA_1\",header=True,mode=\"append\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd6ef20-ebf5-43aa-a0ba-a58b5d96bf02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_frame_7.coalesce(1).write.csv(\"dbfs:/FileStore/BB1/CSV/DATA_2\",header=True,mode=\"append\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a517a2d6-42c3-475c-9675-7c4d5add2d6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create a dir for the types of Data being stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c548c7-1322-4297-bf2f-92a2e10b1a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res6: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res6: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "mkdirs dbfs:/FileStore/BB1/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f352a3-f60b-41a3-83cb-9eb923630336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res8: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res8: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "mkdirs  dbfs:/FileStore/BB1/ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdda50b7-fbb3-49b6-8300-8f917cd99cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[76]: True"
     ]
    }
   ],
   "source": [
    "#%fs \n",
    "#mkdirs /dbfs/FileStore/BB1/Data/CSV\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB1/CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cb8fc7-b261-45a4-b633-c6731b04c6d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB1/CSV/DATA_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bab576-873a-4226-a125-21f3d2bcd035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB1/CSV/DATA_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d25d24-c8aa-414b-b623-655f40f7b0af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##On topof temp view can we create another temp view ? I tried but it failed with some error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e728ea-5c35-4c03-b78e-48694e086f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---+--------+\n|    _c0|      _c1|      _c2|_c3|     _c4|\n+-------+---------+---------+---+--------+\n|4000000|   Apache|    Spark| 11|    null|\n|4000001| Kristina|    Chung| 55|   Pilot|\n|4000001| Kristina|    Chung| 55|   Pilot|\n|4000002|    Paige|     Chen|7-7|   Actor|\n|4000003|   Sherri|   Melton| 34|Reporter|\n|4000003|  mohamed|    irfan| 41|      IT|\n|4000003|vaishnavi|santharam| 30|      IT|\n|4000004| Gretchen|     null| 66|    null|\n|   null|    Karen|  Puckett| 74|  Lawyer|\n|4000006|  Patrick|     Song| 24|    null|\n|    ten|    Elsie| Hamilton| 43|   Pilot|\n|   null|    Hazel|   Bender| 63|    null|\n+-------+---------+---------+---+--------+\nonly showing top 12 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_7=spark.read.csv(\"dbfs:/FileStore/data/custsmodified\" , sep=\",\" , header=False , inferSchema=False,mode=\"dropmalformated\")\n",
    "data_frame_7.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1fb22b-188c-401f-a6b6-61dc2681cbd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---+--------+\n|    _c0|      _c1|      _c2|_c3|     _c4|\n+-------+---------+---------+---+--------+\n|4000000|   Apache|    Spark| 11|    null|\n|4000001| Kristina|    Chung| 55|   Pilot|\n|4000001| Kristina|    Chung| 55|   Pilot|\n|4000002|    Paige|     Chen|7-7|   Actor|\n|4000003|   Sherri|   Melton| 34|Reporter|\n|4000003|  mohamed|    irfan| 41|      IT|\n|4000003|vaishnavi|santharam| 30|      IT|\n|4000004| Gretchen|     null| 66|    null|\n|   null|    Karen|  Puckett| 74|  Lawyer|\n|4000006|  Patrick|     Song| 24|    null|\n|    ten|    Elsie| Hamilton| 43|   Pilot|\n|   null|    Hazel|   Bender| 63|    null|\n+-------+---------+---------+---+--------+\nonly showing top 12 rows\n\n+-------+---------+\n|    _c0|      _c1|\n+-------+---------+\n|4000000|   Apache|\n|4000001| Kristina|\n|4000001| Kristina|\n|4000002|    Paige|\n|4000003|   Sherri|\n|4000003|  mohamed|\n|4000003|vaishnavi|\n|4000004| Gretchen|\n|   null|    Karen|\n|4000006|  Patrick|\n|    ten|    Elsie|\n|   null|    Hazel|\n+-------+---------+\nonly showing top 12 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_7.createOrReplaceTempView(\"table1\")\n",
    "temp_data_frame=spark.sql(\"select * from table1\");\n",
    "temp_data_frame.show(12)\n",
    "\n",
    "data_frame_7.select(\"_c0\",\"_c1\").createOrReplaceTempView(\"table2\")\n",
    "temp_data_frame=spark.sql(\"select * from table2\");\n",
    "temp_data_frame.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c068a52-8005-420d-ada1-6e78ff714d9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###On topof temp view can we create another temp view ? I tried but it failed with some error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5d3c56-ded7-4190-8d65-461f81dbfc39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---+\n|    _c0|      _c1|      _c2|_c3|\n+-------+---------+---------+---+\n|4000000|   Apache|    Spark| 11|\n|4000001| Kristina|    Chung| 55|\n|4000001| Kristina|    Chung| 55|\n|4000002|    Paige|     Chen|7-7|\n|4000003|   Sherri|   Melton| 34|\n|4000003|  mohamed|    irfan| 41|\n|4000003|vaishnavi|santharam| 30|\n|4000004| Gretchen|     null| 66|\n|   null|    Karen|  Puckett| 74|\n|4000006|  Patrick|     Song| 24|\n|    ten|    Elsie| Hamilton| 43|\n|   null|    Hazel|   Bender| 63|\n+-------+---------+---------+---+\nonly showing top 12 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_7.createOrReplaceTempView(\"table1\")\n",
    "spark.sql(\"select _c0,_c1,_c2,_c3 from table1\").createOrReplaceTempView(\"table2\")\n",
    "temp_data_frame=spark.sql(\"select * from table2\");\n",
    "temp_data_frame.show(12)\n",
    "#pyspark.sql.utils.AnalysisException: Inserting into an RDD-based table is not allowed.;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9adfc99-b70b-488a-bf0e-0327385e0b4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Write in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a73624c-8175-407e-8328-dea082ed7821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write in csv \n",
    "data_frame_7.write.csv(\"dbfs:/FileStore/BB1/CSV/DATA\",header=False,mode=\"append\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ed0b1f-b67d-4c38-b78e-cb337cc50aa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write in JSON \n",
    "data_frame_7.write.json(\"dbfs:/FileStore/BB1/JSON/WRITE\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd778a8a-91e5-4294-bd3f-758c0f10ce8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write in ORC\n",
    "data_frame_7.write.orc(\"dbfs:/FileStore/BB1/ORC/DATA\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4869093b-1bea-429d-84c3-3680b816824e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write in parquet \n",
    "data_frame_7.write.parquet(\"dbfs:/FileStore/BB1/PARQUET/DATA\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfa9ef4-2b30-4c82-9212-ca9ff323f3d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Writing in a Hive Table\n",
    "###Using DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac50b023-e828-47c2-b2a6-2b9f02a44cb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/nyse_json_tbl\", True) # Delete if the table is already present \n",
    "data_frame_7.write.saveAsTable(\"default.nyse_json_tbl\",mode=\"append\")\n",
    "data_frame_7.write.insertInto(\"default.nyse_json_tbl\",overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48057213-1d88-45d1-95b6-30c72cd561e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###From file create a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe8d357-b37e-4b22-8270-c954cfaeb52c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n| _c0|_c1|  _c2|\n+----+---+-----+\n|NYSE|CLI| 35.3|\n|NYSE|ABC|24.62|\n|NYSE|CAB| 33.2|\n+----+---+-----+\n\n+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n+--------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_5.show(3)\n",
    "data_frame_5.createOrReplaceTempView(\"table1\");\n",
    "data_frame_5_1 = spark.sql(\"select _c0 as Exchange , _c1 as Stock , _c2 as Price from table1\");\n",
    "data_frame_5_1.show(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7ee01e1-80ad-4615-8cc5-c47311d354d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Creating a table from the data in FIlestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd62744-7e08-42a6-9669-2889612d3583",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##By CTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2259dfa-38fa-4ece-ae4f-a524bbcfbb80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-677537604745238>\", line 2, in <module>\n    temp_df=spark.sql(\"create table default.nyse_table_data row format delimited fields terminated by \",\" as select * from table2\")\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/session.py\", line 1386, in sql\n    litArgs = {k: _to_java_column(lit(v)) for k, v in (args or {}).items()}\nAttributeError: 'str' object has no attribute 'items'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'str' object has no attribute 'items'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_frame_5_1.createOrReplaceTempView(\"table2\");\n",
    "temp_df=spark.sql(\"create table default.nyse_table_data row format delimited fields terminated by \",\" as select * from table2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4a4dbb-a516-4e6e-9612-dc1bd4bfd989",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##The above error is \n",
    "###The error you encountered is due to incorrect SQL syntax and the improper way to create a table and insert data into it. Instead of using the Spark SQL method directly for creating a table with specific properties, you should use the saveAsTable method to save the DataFrame as a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9754cf20-473b-4855-a52d-4a8be95496b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Below we are going to create table and then load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8413b9-8fa2-4411-a5dd-c896b8a506d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n|Exchange|Stock|Price|\n+--------+-----+-----+\n|    NYSE|  CLI| 35.3|\n|    NYSE|  ABC|24.62|\n|    NYSE|  CAB| 33.2|\n|    NYSE|  XYZ| 45.8|\n|    NYSE|  LMN| 56.1|\n|    NYSE|  PQR| 28.9|\n|    NYSE|  UVW| 39.7|\n|    NYSE|  DEF| 48.5|\n|    NYSE|  GHI| 50.3|\n|    NYSE|  JKL| 22.6|\n|    NYSE|  MNO| 34.4|\n|    NYSE|  RST| 29.1|\n|    NYSE|  TUV| 47.2|\n|    NYSE|  WXY| 52.9|\n|    NYSE|  BCD| 41.7|\n|    NYSE|  EFG| 36.3|\n|    NYSE|  HIJ| 40.2|\n|    NYSE|  KLM| 38.4|\n|    NYSE|  NOP| 30.1|\n|    NYSE|  QRS| 43.5|\n|    NYSE|  TUV| 37.8|\n|    NYSE|  WXY| 26.7|\n|    NYSE|  ZAB| 49.0|\n+--------+-----+-----+\nonly showing top 23 rows\n\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"create table default.manual_nyse (Exchange string , Stock string , Price double  ) row format delimited fields terminated by \",\"  \")\n",
    "#the above error is due to indentation \n",
    "spark.sql(\"drop table default.manual_nyse\")\n",
    "spark.sql(\"\"\"create table default.manual_nyse (Exchange string , Stock string , Price double  ) row format delimited fields terminated by '~'  \"\"\")\n",
    "spark.sql(\"\"\"load data local inpath 'dbfs:/FileStore/BB1/CSV/nyse.csv' into table default.manual_nyse \"\"\")\n",
    "\n",
    "data_frame_12 = spark.sql(\"\"\" select * from default.manual_nyse\"\"\")\n",
    "data_frame_12.show(23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49aecb88-ef53-433e-8f5c-2df03f6b2dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##The nyse data is updated so removed and uploaded again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b6ea23-3150-4d48-ab89-f4ce20a2f6df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/BB1/CSV/nyse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8edff1c2-ad17-475b-8cac-51f1548eef83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#END OF BB1 Try the JDBC in OnPrem VirtualMachine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5afd68-da35-49cd-964d-b3dc63852575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 731322617850594,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB1_scratch",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
